# Project: Prompt-Opt - Exploring LLM Prompt Engineering and Evaluation  

## Overview  

This project was initiated to explore techniques for Large Language Model (LLM) prompt engineering and automated evaluation, with an initial focus on building a system to optimize prompts for specific tasks. A key aspect of this exploration involved utilizing Anthropic's API, particularly with the Claude Haiku model, on the GSM8K benchmark for grade-school math word problems.  The primary goal was to establish a baseline performance on GSM8K using a few-shot chain-of-thought prompt with Claude Haiku, and then to investigate methods for iteratively refining this prompt to improve accuracy.  

## Key Findings & Project Evolution  

A significant and somewhat unexpected finding emerged during the baseline evaluation phase: the initial few-shot prompt, when used with the `claude-3-haiku-20240307` model, demonstrated remarkably high accuracy on our development subset of the GSM8K benchmark (achieving approximately 97%).  **This strong initial performance with a relatively light-weight and fast model like Haiku indicated that further extensive iterative optimization aimed *solely* at increasing raw accuracy on this specific benchmark with this prompt structure might yield diminishing returns.**  Consequently, while the framework for prompt evaluation was successfully established, the project's emphasis shifted slightly from aggressive prompt optimization to: 1.  Verifying and understanding this high baseline performance of Claude Haiku. 2.  Documenting the setup for LLM experimentation and evaluation. 3.  Recognizing the impressive capabilities of modern LLMs like Claude Haiku, which can achieve high performance with well-structured few-shot prompts.  This project serves as a practical example of setting up an LLM evaluation pipeline and highlights that sometimes, with advanced models and effective initial prompting, the need for complex automated optimization for a specific task can be less critical than anticipated.  ## Technologies & Tools Used  * **Programming Language:** Python 3.10 * **LLM API:** Anthropic API (specifically `claude-3-haiku-20240307`) * **Benchmark:** GSM8K (Grade School Math 8K) * **Key Python Libraries:**     * `anthropic`: For interacting with the Claude API.     * `datasets`: For downloading and managing the GSM8K benchmark.     * `python-dotenv`: For managing API keys.     * `regex`: For answer extraction. * **Environment Management:** Python `venv`  


## Setup and Usage

1.  **Clone the Repository (if applicable) or Create Project Directory:**
    ```bash
    # git clone ...
    # cd prompt-opt
    ```

2.  **Set up Python Environment:**
    Ensure you have Python 3.10 installed. Then run the setup script:
    ```bash
    chmod +x setup_env.sh
    ./setup_env.sh
    ```
    This will create a virtual environment named `.prompt-opt` and a `requirements.txt` file.

3.  **Activate Virtual Environment:**
    ```bash
    source .prompt-opt/bin/activate
    ```

4.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

5.  **Set up API Key:**
    Create a `.env` file in the project root and add your Anthropic API key:
    ```
    ANTHROPIC_API_KEY="your_anthropic_api_key_here"
    ```

6.  **Download Benchmark Data:**
    Run the Python script responsible for downloading the GSM8K data (e.g., `python download_data.py`). This will populate the `benchmark_data` directory.

7.  **Run Baseline Evaluation:**
    Execute the `prompt_runner.py` script to evaluate the baseline prompt on the GSM8K development set:
    ```bash
    python prompt_runner.py
    ```

## Potential Future Work / Next Steps (Beyond Current Scope)

While extensive prompt optimization for accuracy on this specific Haiku+GSM8K setup was deemed less critical due to high initial performance, several avenues could be explored in a broader context:

* **Verification on Full `test` Set:** Rigorously evaluate the baseline prompt on the complete, official GSM8K `test` split to confirm performance.
* **Optimization for Different Goals:**
    * **Efficiency:** Optimize prompts to achieve similar accuracy with fewer tokens generated by the LLM (reducing cost/latency).
    * **Robustness:** Test and optimize prompts against adversarial inputs or paraphrased questions.
    * **Different Models:** Apply the optimization framework to improve performance for other models (e.g., smaller open-source models or different Claude versions) on GSM8K or other benchmarks.
* **More Challenging Benchmarks:** Apply the prompt engineering and evaluation framework to more difficult reasoning benchmarks where baseline performance is lower.
* **Advanced Optimizer Logic:** Develop a more sophisticated "Optimizer LLM" that can propose a wider variety of prompt modifications or use more structured search techniques.

This project provides a solid foundation for such further explorations into LLM behavior and prompt engineering.

